# -*- coding: utf-8 -*-
"""SVM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18j874Q32Zo1D_4JGBvLelHdbt3V4GV8s

NAME : MOUMITA ROY  ;
REGISTRATION NUMBER: 22MSD7024
"""

import numpy as np
import pandas as pd
df = pd.read_excel("/content/drive/MyDrive/Pumpkin_Seeds_Dataset.xlsx")
df

! pip install openpyxl

# importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

warnings.filterwarnings('ignore')

"""Exploring Dataset and Data Cleaning"""

# checking the shape
df.shape

# printing the first 5 rows of the dataset
df.head()

# checking the number of rows times the number of columns
df.size

# checking for duplicates
df.duplicated().sum()

# checking for missing values
df.isna().sum()

# getting info on the features
df.info()

# getting statistical summary of numerical columns
df.describe()

# getting the median of the columns
df.median()

# getting a list of numerical columns
num_cols = [col for col in df.columns if df[col].dtypes != 'object']
num_cols

def boxplot(df, cols):
    for col in cols:
        sns.set_style('whitegrid')
        sns.boxplot(y=col, data=df)
        plt.title('Boxplot of ' + col)
        plt.ylabel(col) #setting text for y axis
        plt.show()
boxplot(df, num_cols)

# skew and kurtosis values of the variable in the data set
df.skew()

df.kurtosis()

# creating distribution and histogram plot of numerical features
def plotGraph (dataset, feature):
    #plt.style.use('solarize_Light2')
    plt.figure(figsize=(15,10))
    plt.subplot(1,2,1)
    plt.title(f'{feature} Distribution Graph')
    sns.distplot(dataset[feature], color='blue')

    plt.subplot(1,2,2)
    plt.title(f'{feature} Histogram Graph')
    sns.histplot(dataset[feature], color='blue', kde=True, bins=10)

    plt.show()

for col in num_cols:
    plotGraph(df, col)

# correlation coefficient Result
corr_Matrix = df.corr()
corr_Matrix

# creating a heatmap to check for correlation
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_Matrix,annot=True, ax=ax)
plt.show()

# getting the value counts for the feature class
df['Class'].value_counts()

# getting the realtionship between the numerical columns and the class column
for col in num_cols:
        sns.set_style('whitegrid')
        sns.boxplot(x='Class', y=col, data=df)
        plt.title('Boxplot of ' + col)
        plt.ylabel(col)
        plt.show()

"""Creating features and target"""

df.columns

X = df[['Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length',  'Aspect_Ration',
        'Compactness']]
y = df[['Class']]

"""Split data into train and test set"""

from sklearn.model_selection import train_test_split

# splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,
                                                    random_state = 0)
# checking the shape
print(X_train.shape, X_test.shape)

"""Scaling the features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train) # transform the train set
X_test = scaler.transform(X_test) # fit the test set

"""Classifying Data with SVM

We will use the SVC library. With SVC, we pass the values of kernel, gamma and C along with other parameters.
"""

# import SVC classifier
from sklearn.svm import SVC

# import metrics to compute accuracy
from sklearn.metrics import accuracy_score

# instantiate classifier with default hyperparameters C=1.0, kernel=rbf and gamma=auto
svc=SVC()

# fit classifier to training set
svc.fit(X_train,y_train)

# make predictions on test set
y_pred=svc.predict(X_test)

# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

# getting train accuracy
train_pred = svc.predict(X_train)
print(f'train accuracy: {accuracy_score(y_train, train_pred)}')

# claffication report
from sklearn.metrics import classification_report
print('Train report')
print(classification_report(y_train, train_pred))
print('Test report')
print(classification_report(y_test, y_pred))

"""Hyperparameter optimization using Gridsearch"""

# importing grid search
from sklearn.model_selection import GridSearchCV

# declaring parameters for hyperparameter tuning
parameters = [ {'C':[1, 10, 100, 1000], 'kernel':['linear']},
               {'C':[1, 10, 100, 1000], 'kernel':['rbf'],
                'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},
               {'C':[1, 10, 100, 1000], 'kernel':['poly'], 'degree': [2,3,4] ,
                'gamma':[0.01,0.02,0.03,0.04,0.05]}]

grid_search = GridSearchCV(estimator = svc,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 5,
                           verbose=0)

grid_search.fit(X_train, y_train)

# examining the best model

# best score achieved during the GridSearchCV
print('GridSearch CV best score : {:.4f}\n\n'.format(grid_search.best_score_))


# print parameters that give the best results
print('Parameters that give the best results :','\n\n', (grid_search.best_params_))


# print estimator that was chosen by the GridSearch
print('\n\nEstimator that was chosen by the search :','\n\n', (grid_search.best_estimator_))

# calculating GridSearch CV score on test set

print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))

svc=SVC(C=100, gamma=0.6)

# fit classifier to training set
svc.fit(X_train,y_train)

# make predictions on test set
y_pred=svc.predict(X_test)

# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

print(classification_report(y_test, y_pred))